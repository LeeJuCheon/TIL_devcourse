# 기계학습과 수학

1. 선형대수학
* 벡터
  * 샘플을 feature vector로 표현
  * 요소의 종류와 크기 표현
* 행렬
  * 여러 개의 벡터를 담음
  * 훈련집합을 담은 행렬을 설계행렬(design matrix)라고 부름
  * 전치행렬 : 행과 열의 요소를 바꾼 행렬
  * 컴퓨터의 연산은 일반 방정식 보다 행렬 형식을 훨씬 빠르고 간결하게 계산한다
  * 행렬의 곱셉은 교환X, 분배O, 결합O
  * 벡터의 내적은 dot으로 표현, a^Tb = a·b
* 텐서(tensor)
  * 3차원 이상의 구조를 가진 숫자 배열
* 유사도(similarity)
  * 벡터를 기하학적으로 해석
  * 코사인 유사도(cosine similarity)
  > ![](images/2022-04-13-13-29-58.png)
  > * -1과 1사이의 값, 1이면 방향이 같은 벡터, -1이면 반대
* 놈(norm)
  * 벡터와 행렬의 거리(크기)
  * 벡터의 p차 놈(1차 : 맨하탄, 2차 : 유클리디안)
  * 행렬의 프로베니우스 놈
* 퍼셉트론(perceptron)
  * 1958년 고안한 분류기 모델
  * 추론 : 학습을 마친 알고리즘을 현장의 세로운 데이터에 적용하는 작업
  * 훈련 : 훈련집합의 샘플에 대해 가장 잘 만족하는 W를 찾아내는 작업
  * 최근 딥러닝에서는 다층 퍼셉트론을 적용
  * 행렬의 필요충분 조건
    * 역행렬을 가진다
    * 특이행렬이 아니다
    * 최대 계수(Rank)를 가진다
    * 모든 행이 선형 독립이다
    * 모든 열이 선형 독립이다
    * A의 행렬식(determinant)는 0이 아니다
    * A^TA는 양의 정부호 대칭 행렬이다
    * A의 고유값(eigen value)은 모두 0이 아니다.
  * Determinant 수치의 의미
    * 0 : 하나의 차원을 따라 축소되어 부피를 상실
    * 1 : 부피 및 방향 보존
    * -1 : 부피는 유지되지만 방향은 보존 안됨
    * 기타 양수 a : a배 부피 확장되며 방향 보존 
  
  * 고윳값과 고유벡터, 고유분해 수식 이해(공학수학 9장)
    * 고유분해는 정사각행렬에만 적용되므로 한계가 존재
  * 특이값 분해(SVD)
    * 정사각행렬이 아닌 행렬의 역행렬을 계산하는데 사용됨
  
2. 확률과 통계
* 확률분포(probability distribution)
  * 확률질량함수(이산확률 변수)
  * 확률밀도함수(연속확률 변수)
* 확률벡터 : 확률변수를 요소로 가지는 벡터
> 키워드
> * 조건부확률
> * Chain rule
> * 독립
> * 조건부 독립
> * 기대값
> * 베이즈 정리
> * MLE
> * 공분산 행렬
> * 가우시안 분포 = 정규분포
> * 베르누이 분포
> * 이항 분포
> * 지수 분포
> * 라프라스 분포
> * 디랙 분포?
> * 혼합 분포들
> * 변수 변환
해당 키워드들은 응용통계 1&2에서 추가적으로 학습



* 다층 퍼셉트론에서 자주 사용되는 활성함수인 시그모이드 함수는 일반적으로 베르누이 분포의 매개변수를 조정을 통해 얻어짐
* 소프트플러스 함수 -> 정규분포 매개변수 조정



3. 정보이론
* 불확실성을 정령화하여 정보이론 방법을 머신러닝에 활용(e.g. 엔트로피, cross 엔트로피, KL 발산 등)
* 사건이 지는 정보를 정량화 할 수 있나? 라는 아이디어에서 출발
* 기본원리 : 자주 발생하는 사건보다 잘 일어나지 않는 사건의 정보량이 많음

* 자기정보(self information)
  * 사건(메세지) e의 정보량
  * -logP(e)
* 엔트로피
  * 확률변수 x의 불확실성을 나타내는 엔트로피
  * 모든 사건 정보량의 기대 값으로 표현
  * 예측이 어려울수록(무질서하고 불확실성이 클수록) 엔트로피가 높음
  ![](images/2022-04-13-23-56-37.png)
* 교차 엔트로피
  * 딥러닝의 손실함수로 많이 사용됨
  * 엔트로피를 최대한 줄이는 방향으로 학습

* KL 다이버전스
  * 두 확률분포 사이의 거리를 계산할 때 주로 사용
* 최적화
  * training set에 따라 정해지는 목적함수의 최저점으로 만드는 parameter를 찾아야함
  * 주로 SGD(stochasstic gradient descent) 사용
  * 낱낱탐색(exhaustive search) 알고리즘
    * 차원만 조금 증가시켜도 불가능
  * 무작위 탐색(random search): 전략 X

* 쉽게 이해하는 Optimizer 도표
  ![](images/2022-04-14-00-41-26.png)

> 찾아볼 키워드
> * 헤세 행렬

4. 딥러닝 실습
* CPU vs GPU
  * GPU는 단순한 작업에 매우 특화
* TPU
  * 구글, Nvidia에서 만든 딥러닝에 특화된 장비, 구글 COLAB에도 지원
* 프레임워크 : 텐서플로우, 파이토치
* PyTorch
  * 텐서를 사용, array와 유사하지만 GPU에서 구동됨
  * Autograd : 텐서를 쌓는 패키지
  * Module : 뉴럴 네트워크 레이어
  * torch.nn 형태의 명령어를 주로 사용 -> 사용자 정의 모듈로 커스터마이징 가능
  * DataLoader : 배치 사이즈,데이터 셋 설정
    * epoch안에 여러 배치로 쪼개 진행
  * Pretrained Models : 기존에 이미 학습되어있는 모델, 주로 alexnet, vgg16, resnet101을 가볍게 사용하는 경우가 있음(import torchvision)
  * Visdom : 시각화 툴
  * tensorboardX : 시각화 툴
  * Dynamic Computation graph, 최근 추세
* Tensor Flow
  * Keras(high-level wrapper)를 사용하여 편리하게 사용하는 추세
  * Distributed Version : 각각 다른 곳에서 분산 가동 가능